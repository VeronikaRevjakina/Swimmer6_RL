{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import suite\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import clone_model\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swimmer6 snake , task: get to the goal \n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(agent):\n",
    "    agent.critic_model.save('Swimmer6/critic_model.h5')\n",
    "    agent.actor_model.save('Swimmer6/actor_model.h5')\n",
    "    agent.target_critic_model.save('Swimmer6/target_critic_model.h5')\n",
    "    agent.target_actor_model.save('Swimmer6/target_actor_model.h5')\n",
    "    \n",
    "def save_scores(scores,last_rewards,first_rewards):\n",
    "    np.savetxt('Swimmer6/scores.csv', scores)\n",
    "    np.savetxt('Swimmer6/last_rewards.csv', last_rewards)\n",
    "    np.savetxt('Swimmer6/first_rewards.csv', first_rewards) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('joints', Array(shape=(5,), dtype=dtype('float64'), name='joints')), ('to_target', Array(shape=(2,), dtype=dtype('float64'), name='to_target')), ('body_velocities', Array(shape=(18,), dtype=dtype('float64'), name='body_velocities'))])\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change all layers to have dtype float64 by default\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(observation):\n",
    "    return np.hstack(\n",
    "        [observation['joints'], observation['to_target'], observation['body_velocities']]\n",
    "    ).reshape((1,25))  # all of the components: joints, to_target, body_velocities sum up to 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    \n",
    "    def __init__(self, state_dim, action_spec, discount_rate,tau, batch_size):\n",
    "        self.action_spec = action_spec\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = self.action_spec.shape[0]\n",
    "        self.action_min = self.action_spec.minimum\n",
    "        self.action_max = self.action_spec.maximum\n",
    "        self.actor_opt = tf.optimizers.Adam(1e-3)   \n",
    "        self.discount_rate = discount_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_scale = 1.0\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.memory = {\n",
    "            \"state\":np.array([]), \n",
    "            \"action\":np.array([], dtype = int), \n",
    "            \"reward\":np.array([]),\n",
    "            \"new_state\":np.array([]), \n",
    "            \"done\":np.array([])\n",
    "            }\n",
    "\n",
    "        # Actor Network\n",
    "        self.actor_model = self.create_actor_model()\n",
    "\n",
    "        #Create target actor as clone of actor\n",
    "        self.target_actor_model = clone_model(self.actor_model)\n",
    "        self.target_actor_model.compile(optimizer='sgd', loss='mse')\n",
    "               \n",
    "        # Critic Network\n",
    "        self.critic_model = self.create_critic_model()\n",
    "        \n",
    "        #Create target critic as clone of critic\n",
    "        self.target_critic_model = clone_model(self.critic_model)\n",
    "        self.target_critic_model.compile(optimizer='sgd', loss='mse')\n",
    "        \n",
    "\n",
    "    def create_actor_model(self):\n",
    "        #Create actor\n",
    "        actor_model = tf.keras.Sequential([\n",
    "        # Add 400 layer with relu on observations input\n",
    "        layers.Dense(400, activation='relu', input_shape=(self.state_dim,)),\n",
    "        # Add 300 hidden layer\n",
    "        layers.Dense(300, activation='relu'),\n",
    "        # Add output action layer with tanh as need 5 size array with -1 to 1 values\n",
    "        layers.Dense(self.action_dim, activation='tanh')])\n",
    "\n",
    "        # config the model with losses and metrics\n",
    "        actor_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse')\n",
    "        return actor_model\n",
    "    \n",
    "    def create_critic_model(self):\n",
    "        #Create critic \n",
    "        #Add inputs observations and actions\n",
    "        action_input = tf.keras.Input(shape=(self.action_dim,), name='action')\n",
    "        observation_input = tf.keras.Input(shape=self.state_dim, name='state')\n",
    "        flattened_observation = layers.Flatten()(observation_input)\n",
    "        #Add layer on observation input only\n",
    "        l=layers.Dense(400, activation='relu', input_shape=(self.state_dim,))(flattened_observation)\n",
    "        #Later add actions\n",
    "        l=layers.Concatenate()([l, action_input])\n",
    "        l=layers.Dense(300, activation='relu')(l)\n",
    "        #Output q value\n",
    "        l=layers.Dense(1, activation='linear')(l)\n",
    "        critic_model = tf.keras.Model(inputs=[action_input, observation_input], outputs=l)\n",
    "\n",
    "        # config the model with losses and metrics\n",
    "        critic_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss='mse')\n",
    "        return critic_model\n",
    "    \n",
    "    def get_action_with_noise(self, obs):\n",
    "        #Make action based on determenistic trained policy, but add Gaussian noise to explore env\n",
    "        action = self.actor_model.predict(obs)[0]\n",
    "        action = action + self.noise_scale*np.random.normal(0,1,size = self.action_dim)\n",
    "        action = np.clip(action, -1.0, 1.0)  # between action values\n",
    "        return action\n",
    "    \n",
    "    def update_buffer(self, obs, new_obs, action, reward, done):\n",
    "        if len(self.memory[\"state\"])>0:\n",
    "            if self.memory[\"done\"].shape[0]>1000000:\n",
    "                for key in self.memory.keys():\n",
    "                    self.memory[key] = self.memory[key][-900000:] #leave only part of buffer\n",
    "            self.memory[\"state\"] = np.vstack((self.memory[\"state\"], obs))\n",
    "            self.memory[\"new_state\"] = np.vstack((self.memory[\"new_state\"], new_obs))\n",
    "            self.memory[\"action\"] = np.vstack((self.memory[\"action\"], action))\n",
    "        else: \n",
    "            # if not initialized new\n",
    "            self.memory[\"state\"] = np.array(obs)\n",
    "            self.memory[\"new_state\"] = np.array(new_obs)\n",
    "            self.memory[\"action\"] = np.array(action)\n",
    "        self.memory[\"reward\"] = np.append(self.memory[\"reward\"], reward)  # not cleaning, always not empty\n",
    "        self.memory[\"done\"] = np.append(self.memory[\"done\"], done)   \n",
    "        \n",
    "    def ddpg_batch_update(self):\n",
    "        if self.memory[\"done\"].shape[0]>=self.batch_size: #there is enough for sampling\n",
    "            indexes = random.sample(range(0, self.memory[\"done\"].shape[0]), self.batch_size)\n",
    "            batch = {\n",
    "                    \"state\": np.squeeze(self.memory[\"state\"][indexes]), \n",
    "                    \"action\": np.squeeze(self.memory[\"action\"][indexes]), \n",
    "                    \"reward\": self.memory[\"reward\"][indexes], \n",
    "                    \"new_state\": np.squeeze(self.memory[\"new_state\"][indexes]), \n",
    "                    \"done\": self.memory[\"done\"][indexes]\n",
    "                    }\n",
    "            \n",
    "            # Q' in DDPG pseudocode to count target critic value using only target networks,\n",
    "            #based on new state choose action by target actor deterministic \n",
    "            #and count q value based on new state and chosen action by target critic \n",
    "            target_q = self.target_critic_model({\"state\":batch[\"new_state\"], \n",
    "                                            \"action\": self.target_actor_model(batch[\"new_state\"])})\n",
    "            # y_i in DDPG  , target value to count MSE: ri+ discount_rate*Q_target \n",
    "            # if done do not add future reward, so added  *(1-batch[\"done\"])\n",
    "            y = batch[\"reward\"].reshape(self.batch_size,1) + \\\n",
    "                (self.discount_rate*(1-batch[\"done\"])).reshape(self.batch_size,1)*target_q\n",
    "           \n",
    "            # update critic by minimizing MSE loss between y and usual critic, actor output\n",
    "            self.critic_model.train_on_batch({\"state\": batch[\"state\"], \"action\": batch[\"action\"]}, y)\n",
    "\n",
    "            # update actor policy using sampled gradient\n",
    "            with tf.GradientTape() as tape:\n",
    "                q = self.critic_model({\"state\": batch[\"state\"], #q vector from critic\n",
    "                                       \"action\": self.actor_model(batch[\"state\"])})  # get action from actor \n",
    "                actor_loss = - tf.reduce_mean(q) # 1 value =mean q value across all q vector values, ignore structure\n",
    "            # compute gradients (loss, variables)\n",
    "            actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_weights)\n",
    "            # update optimizer applying counted gradients\n",
    "            self.actor_opt.apply_gradients(zip(actor_grads, self.actor_model.trainable_weights))\n",
    "#             self.actor_opt.minimize(actor_loss, self.actor_model.trainable_weights)\n",
    "\n",
    "            # update target networks target=tau*net + (1-tau)*target\n",
    "            self.target_critic_model.set_weights((1.0-self.tau) * np.array(self.target_critic_model.get_weights()) + \\\n",
    "                                          self.tau*np.array(self.critic_model.get_weights()))\n",
    "            self.target_actor_model.set_weights((1.0-self.tau) * np.array(self.target_actor_model.get_weights()) + \\\n",
    "                                         self.tau*np.array(self.actor_model.get_weights()))\n",
    "            \n",
    "            self.noise_scale*=0.99  # lower noise as exploration/exploitation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = env.action_spec()\n",
    "action_dim = action_spec.shape[0]\n",
    "action_low=action_spec.minimum  # [-1. -1. -1. -1. -1.]\n",
    "action_high=action_spec.maximum  # [1. 1. 1. 1. 1.]\n",
    "state_dim = 25 # np.hstack all of the components: joints, to_target, body_velocities\n",
    "# agent = DDPG(state_dim, action_spec,0.99,0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env, num_games, state_dim, action_spec):\n",
    "    #initalize empty\n",
    "    scores = np.array([])\n",
    "    last_rewards = np.array([])\n",
    "    first_rewards = np.array([])\n",
    "    action_spec = env.action_spec()\n",
    "    \n",
    "    # initialize with network models + target and empty memory\n",
    "    agent = DDPG(state_dim, action_spec, 0.99,0.01, 64)\n",
    "    start = time.time()\n",
    "\n",
    "    for game in range(1, num_games+1):       \n",
    "        if (game % 25) == 0:  # after each ... iterarion number store data\n",
    "            #temporary save all 4 models if disconnected helpful\n",
    "            save_models(agent)\n",
    "            save_scores(scores,last_rewards,first_rewards)       \n",
    "            print(f\"{iteration} number iterations left\")\n",
    "            print(f\"{(time.time()-start)/60} minutes remaining\")\n",
    "            \n",
    "        #just plot for demo     \n",
    "        if (game % 100) == 0:\n",
    "            x=list(range(len(scores)))\n",
    "            plt.plot(x, scores)\n",
    "            plt.xlabel(\"Iteration number\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.show()\n",
    "\n",
    "        time_step = env.reset()  # reset env once in game\n",
    "        obs = convert_state(time_step.observation) # to np array based on env obs \n",
    "\n",
    "        game_score = 0\n",
    "        #for iterations in game, fixed 1000 for env, automatically returns last\n",
    "        while not time_step.last():       \n",
    "            # get action with added noise\n",
    "            action = agent.get_action_with_noise(obs)\n",
    "            # make action \n",
    "            time_step = env.step(action)\n",
    "            #convert got from env obs\n",
    "            new_obs = convert_state(time_step.observation)            \n",
    "            # update info\n",
    "            game_score += time_step.reward \n",
    "            #last is done flag\n",
    "            agent.update_buffer(obs, new_obs, action, time_step.reward, 0)\n",
    "            obs = new_obs\n",
    "            # update networks weights\n",
    "            agent.ddpg_batch_update()\n",
    "            #store info if memory full then will clean first rewards, need for stats\n",
    "            if len(first_rewards)==len(last_rewards):\n",
    "                first_rewards = np.append(first_rewards, time_step.reward)\n",
    "        #gane ended, store last step\n",
    "        new_obs = convert_state(time_step.observation)   \n",
    "        # done flag =1\n",
    "        agent.update_buffer(obs, new_obs, action, time_step.reward, 1)\n",
    "        # update networks weights\n",
    "        agent.ddpg_batch_update()\n",
    "        #last reward added\n",
    "        game_score += time_step.reward \n",
    "        \n",
    "        #across all games scores array\n",
    "        scores = np.append(scores, game_score)\n",
    "        last_rewards = np.append(last_rewards, time_step.reward)\n",
    "        #after each game\n",
    "        print(f\"Iteration: {game};   score: {game_score}; game reward: {time_step.reward}; previous game reward: {first_rewards[-1]}\")\n",
    "    \n",
    "    #after all games complete save trained networks\n",
    "    save_models(agent)\n",
    "    save_scores(scores,last_rewards,first_rewards)\n",
    "    return agent, scores, last_rewards, first_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1;   score: 122.17612617358806; game reward: 0.12426098087077711; previous game reward: 0.08716617710255155\n"
     ]
    }
   ],
   "source": [
    "num_games = 1000\n",
    "\n",
    "agent,scores, last_rewards, first_rewards = train_model(env, num_games, state_dim, action_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.84270279515664\n",
      "174.37853653624165\n",
      "990.3867739230217\n",
      "76.40498741416086\n",
      "11.85914494957288\n",
      "7.8378229237979244\n",
      "379.93543975807563\n",
      "998.9540249895206\n",
      "9.279951886452674\n",
      "186.781442120145\n",
      "29.917483871047192\n",
      "18.841813762040342\n",
      "719.792014475365\n",
      "805.2113464955687\n",
      "6.066806933490635\n",
      "471.60333387103304\n",
      "7.894880668466012\n",
      "10.260713640206554\n",
      "441.04042191833634\n",
      "17.162429921387908\n",
      "9.984490586754552\n",
      "497.2597006810599\n",
      "10.402056148713594\n",
      "9.167138935953089\n",
      "18.75408767159369\n",
      "14.042000761098931\n",
      "18.27572731557042\n",
      "11.661311304978614\n",
      "29.211435875778346\n",
      "8.682749542307647\n",
      "11.11649064771654\n",
      "14.991746371904334\n",
      "11.35004667664715\n",
      "7.167194815978092\n",
      "442.93244975917196\n",
      "13.443347133179362\n",
      "20.974426910764954\n",
      "8.369623629784828\n",
      "10.276102603329674\n",
      "4.007666418266852\n",
      "38.43273675369839\n",
      "10.065484949903052\n",
      "665.3867561239139\n",
      "46.10872632572045\n",
      "985.6128515787769\n",
      "46.709100646778026\n",
      "17.2076953475195\n",
      "10.766706531839262\n",
      "14.512184854906188\n",
      "17.205131684222888\n",
      "13.909490875054967\n",
      "42.98485235124309\n",
      "493.99112416622785\n",
      "4.727411009707003\n",
      "8.052574964055301\n",
      "42.47589954521849\n",
      "6.862096649752367\n",
      "5.803302297068805\n",
      "983.4257428189279\n",
      "24.81025890252985\n",
      "18.725254063625677\n",
      "5.972139867408337\n",
      "829.6042041338737\n",
      "8.30286896179115\n",
      "59.312162764709136\n",
      "14.231889406524921\n",
      "8.244552671977148\n",
      "20.39987507038214\n",
      "15.958713947076038\n",
      "5.6131082443873765\n",
      "24.076906313265596\n",
      "75.15690178103017\n",
      "1000.0\n",
      "27.276498311087966\n",
      "54.86091340982325\n",
      "16.66439472556119\n",
      "782.6251639282196\n",
      "6.711084387890938\n",
      "760.0089262084408\n",
      "7.642154372004136\n",
      "22.33035912433914\n",
      "7.790833044654096\n",
      "5.516705832248797\n",
      "15.69792778375675\n",
      "38.546259565641535\n",
      "12.315069873158038\n",
      "9.068189380601856\n",
      "10.664301778129126\n",
      "4.35311254954725\n",
      "7.651553530657213\n",
      "405.23399201548284\n",
      "9.309123766974245\n",
      "9.216561126957123\n",
      "7.812995485329377\n",
      "77.30452549463537\n",
      "907.0001257055488\n",
      "11.789459827255566\n",
      "434.78977117273524\n",
      "9.084539611871383\n",
      "30.71445446307562\n",
      "Average reward on test 100 games:  158.0414956872238\n"
     ]
    }
   ],
   "source": [
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import numpy as np\n",
    "\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\",visualize_reward = True)\n",
    "action_spec = env.action_spec()\n",
    "obs_spec = env.observation_spec()\n",
    "\n",
    "test_games_num = 100\n",
    "test_scores = []\n",
    "\n",
    "for game in range(test_games_num):\n",
    "    score = 0\n",
    "    step_data = env.reset()  # get initial state\n",
    "    \n",
    "    while not step_data.last():\n",
    "        action = np.random.uniform(low=action_spec.minimum,high=action_spec.maximum,size=action_spec.shape)\n",
    "        step_data = env.step(action)\n",
    "        score += step_data.reward\n",
    "    print(score)   \n",
    "    test_scores.append(score)\n",
    "\n",
    "print(\"Average reward on test 100 games: \", np.mean(test_scores))\n",
    "    \n",
    "#     for step in range(max_steps_in_game):\n",
    "# #         env.render()  # show gif\n",
    "# #         print(observation)  # print state vector 8, fixed for environment\n",
    "#         obs = np.reshape(obs, (1, state_size))\n",
    "#         rewards = trained_model.predict(obs)\n",
    "#         action = int(np.argmax(rewards[0]))\n",
    "# #         action = env.action_space.sample()\n",
    "#         obs, reward, done, info = env.step(action)  # step returns 4 parameters\n",
    "#         score +=reward\n",
    "#         if done:  # game over need reset\n",
    "# #             print(\"Episode finished after {} timesteps\".format(step+1))\n",
    "#             print(score)\n",
    "#             break\n",
    "#     test_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewer\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import numpy as np\n",
    "\n",
    "# env = suite.load(domain_name=\"humanoid\", task_name=\"stand\")\n",
    "env = suite.load(domain_name=\"swimmer\", task_name=\"swimmer6\")\n",
    "action_spec = env.action_spec()\n",
    "print(action_spec)\n",
    "\n",
    "# Define a uniform random policy.\n",
    "def random_policy(time_step):\n",
    "#     del time_step  # Unused.\n",
    "    print(time_step)\n",
    "    \n",
    "    action = np.random.uniform(low=action_spec.minimum,\n",
    "                           high=action_spec.maximum,\n",
    "                           size=action_spec.shape)\n",
    "    print(action)\n",
    "    return action\n",
    "# time_step = env.reset()\n",
    "\n",
    "# Launch the viewer application.\n",
    "viewer.launch(env, policy=random_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
